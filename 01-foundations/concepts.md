# How LLMs Work

## The 30-Second Explanation

Large Language Models are neural networks trained on massive text corpora to predict the next token in a sequence. They learn statistical patterns in language â€” grammar, facts, reasoning patterns, code structure â€” and generate text by repeatedly predicting "what comes next" given everything before it.

---

## Transformers

The transformer architecture (Vaswani et al., 2017 â€” "Attention Is All You Need") is the foundation of all modern LLMs.

**Key idea:** Instead of processing text sequentially (like RNNs), transformers process all tokens in parallel and use *attention* to learn which tokens are relevant to each other.

**Architecture overview:**

```
Input Text
    â†“
Tokenization â†’ Token IDs â†’ Token Embeddings + Positional Encodings
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Transformer Block (Ã—N)    â”‚  â† Stack of identical layers
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Multi-Head Attention  â”‚  â”‚  â† "Which tokens matter for this token?"
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Feed-Forward Network â”‚  â”‚  â† "What does this combination mean?"
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  + Layer Norm + Residuals   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Output Probabilities (over entire vocabulary)
```

**Why it matters for interviews:** Understanding transformers lets you reason about context window limits, why certain tasks are hard (e.g., counting), and why prompt structure matters.

---

## Tokenization

LLMs don't see characters or words â€” they see **tokens**, which are subword units learned from the training data.

**How it works:**
- Algorithms like BPE (Byte Pair Encoding) or SentencePiece build a vocabulary by merging frequent character pairs
- Common words become single tokens; rare words get split into pieces
- Typical vocabulary: 32Kâ€“100K tokens

**Examples (approximate, varies by model):**
```
"Hello world"     â†’ ["Hello", " world"]           (2 tokens)
"tokenization"    â†’ ["token", "ization"]           (2 tokens)
"ğŸ‰"             â†’ ["ğŸ‰"]                         (1 token, sometimes more)
"XMLHttpRequest"  â†’ ["XML", "Http", "Request"]     (3 tokens)
```

**Why it matters:**
- **Cost** â€” you're billed per token (input + output)
- **Context limits** â€” measured in tokens, not words (~0.75 words per token for English)
- **Edge cases** â€” tokenization affects math (numbers split unpredictably), code (whitespace-sensitive), and non-English text (more tokens per word)

---

## Attention Mechanism

Attention is *the* core innovation that makes transformers work. It lets each token "look at" every other token and decide how much to weight each one.

**Self-attention in plain English:**

For the input "The cat sat on the mat because **it** was tired":
- The token "it" needs to figure out what it refers to
- Attention computes a relevance score between "it" and every other token
- "cat" gets a high score â†’ the model understands "it" = "cat"

**How it's computed:**

Each token produces three vectors:
- **Query (Q):** "What am I looking for?"
- **Key (K):** "What do I contain?"
- **Value (V):** "What information do I provide?"

```
Attention(Q, K, V) = softmax(QKáµ€ / âˆšd_k) Ã— V
```

The `QKáµ€` dot product measures relevance. Softmax normalizes into weights. Those weights are applied to Values to produce the output.

**Multi-head attention:** Run multiple attention computations in parallel (each "head" can focus on different relationship types â€” syntax, semantics, position, etc.).

**Why it matters:** Attention is why prompt structure matters. Placing instructions near relevant content, using clear delimiters, and structuring prompts well all help the model attend to the right information.

---

## Context Windows

The **context window** is the maximum number of tokens the model can process in a single request (input + output combined).

**Current landscape (approximate):**
| Model Family | Context Window |
|---|---|
| GPT-4o | 128K tokens |
| Claude 3.5+ | 200K tokens |
| Gemini 1.5 Pro | 1Mâ€“2M tokens |
| Llama 3 | 8Kâ€“128K tokens |

**Key concepts:**

- **Input tokens** â€” your prompt, system message, conversation history
- **Output tokens** â€” the model's response (often capped separately, e.g., 4Kâ€“8K)
- **Lost in the middle** â€” models tend to pay more attention to the beginning and end of the context; information in the middle can be overlooked
- **Effective vs. stated context** â€” a model may accept 128K tokens but perform degraded on tasks requiring precise recall beyond ~32K

**Why it matters:**
- Determines how much history/context you can include
- Directly affects RAG design (how many retrieved chunks fit)
- Long contexts cost more and increase latency

---

## Temperature & Sampling

After the model computes probabilities for the next token, **sampling parameters** control how that distribution is converted into an actual token choice.

### Temperature

Controls randomness. Technically, it scales the logits before softmax:

```
P(token_i) = exp(logit_i / T) / Î£ exp(logit_j / T)
```

| Temperature | Behavior | Use Case |
|---|---|---|
| 0 | (Near-)deterministic, highest probability token | Factual Q&A, classification, structured output |
| 0.3â€“0.7 | Balanced creativity and coherence | General conversation, writing |
| 1.0+ | High randomness, more diverse/surprising outputs | Creative writing, brainstorming |

### Top-p (Nucleus Sampling)

Instead of considering all tokens, only sample from the smallest set whose cumulative probability â‰¥ p.

- `top_p=0.9` â†’ consider tokens comprising the top 90% of probability mass
- Dynamically adjusts the candidate pool â€” narrow for confident predictions, wider for uncertain ones

### Top-k

Only consider the top k most probable tokens. Simpler but less adaptive than top-p.

### In Practice

- **Deterministic tasks** (extraction, classification): temperature=0
- **Creative tasks**: temperature=0.7â€“1.0, top_p=0.9
- Don't set both temperature and top_p aggressively â€” they compound

---

## Embeddings

Embeddings are dense vector representations of text in a high-dimensional space where **semantic similarity maps to geometric proximity**.

**Key properties:**
- Typical dimensions: 256â€“3072 (depends on model)
- Similar meanings â†’ nearby vectors (measured by cosine similarity)
- Capture semantic meaning, not just lexical overlap

**Example:**
```
embed("king") - embed("man") + embed("woman") â‰ˆ embed("queen")

cosine_similarity(embed("dog"), embed("puppy"))   â‰ˆ 0.92  (high)
cosine_similarity(embed("dog"), embed("bicycle"))  â‰ˆ 0.15  (low)
```

**Where they're used:**
- **Semantic search** â€” find relevant documents by meaning, not keywords
- **RAG** â€” retrieve context to augment LLM prompts
- **Clustering** â€” group similar content
- **Classification** â€” use as features for downstream models

**Embedding models vs. LLMs:**
- Embedding models (e.g., OpenAI `text-embedding-3-small`, Cohere `embed-v3`) are specialized for producing good vectors
- LLMs generate text; embedding models map text â†’ vectors
- Much cheaper to run than LLMs

---

## Training Pipeline (High Level)

Understanding the training stages helps explain model behavior:

### 1. Pre-training
- Train on massive internet text (books, web, code)
- Objective: predict the next token
- Result: a model that can complete text but isn't "helpful"
- This is where most factual knowledge is learned

### 2. Supervised Fine-Tuning (SFT)
- Train on curated (prompt, response) pairs
- Teaches the model to follow instructions and be helpful
- Relatively small dataset compared to pre-training

### 3. RLHF / RLAIF (Alignment)
- **RLHF:** Reinforcement Learning from Human Feedback
- **RLAIF:** ...from AI Feedback
- Human raters rank model outputs â†’ train a reward model â†’ optimize the LLM against it
- This is what makes models refuse harmful requests, be honest about uncertainty, etc.

### 4. (Optional) Domain Fine-Tuning
- Further fine-tune on domain-specific data
- Useful for specialized tasks (legal, medical, code)

**Why it matters:** Training explains why models hallucinate (pre-training optimizes for plausible text, not truth), why they're "helpful" (SFT + RLHF), and the limits of fine-tuning vs. prompting.

---

## Key Mental Models

1. **LLMs are probabilistic text completers** â€” not databases, not reasoning engines. Their "reasoning" emerges from pattern completion over training data.

2. **The context window is the model's working memory** â€” everything it needs must be in the prompt or its weights. It has no persistent memory between calls.

3. **Tokens are the atomic unit** â€” cost, limits, and many edge cases tie back to tokenization.

4. **Garbage in, garbage out** â€” prompt quality directly determines output quality. The model is extremely sensitive to how you ask.

5. **Models don't "know what they don't know"** â€” they generate confident-sounding text even when wrong (hallucination). Always verify factual claims for critical applications.
